{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcbc1ea5-8299-443d-9c3c-4455688f8a3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import uuid\n",
    "from delta.tables import *\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from sodapy import Socrata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dddc69a-35bd-402a-b2f4-9417a4d190fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"env\", \"dev\", [\"dev\", \"prod\"], \"Execution Environment\")\n",
    "dbutils.widgets.dropdown(\"source_data\", \"yellow_tripdata\", [\"yellow_tripdata\", \"green_tripdata\"], \"NYCOpenData\")\n",
    "\n",
    "env = dbutils.widgets.get('env')\n",
    "source_data = dbutils.widgets.get('source_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9d5b943-e7eb-412a-9114-2d65e93ec7f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criação dos Schemas\n",
    "\n",
    "def create_schemas(catalog: str, schemas: list[str]):\n",
    "    \"\"\"\n",
    "    Criação dos schemas no respectivo catálogo.\n",
    "\n",
    "    Args:\n",
    "        - catalog(str): nome do catálogo.\n",
    "        - schemas(list[str]): lista com o nome dos schemas a serem criados.\n",
    "    \"\"\"\n",
    "\n",
    "    for schema in schemas:\n",
    "\n",
    "        full_schema = f\"{catalog}.{schema}\"\n",
    "        print(f\"Garantindo que o respectivo Schema existe: {full_schema}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {full_schema}\")\n",
    "\n",
    "schemas = [\"bronze\", \"silver\", \"metadata\"]\n",
    "catalog = \"main\"\n",
    "\n",
    "create_schemas(catalog, schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb07d8d3-20ba-4383-9aa8-a546528a8605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Recuperando os segredos\n",
    "\n",
    "scope = 'db-scope'\n",
    "aws_account_id = dbutils.secrets.get(scope, 'aws_account_id')\n",
    "nyc_app_token = dbutils.secrets.get(scope, 'nyc_app_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca1168be-78ff-4e2b-81ab-8e440948da2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definindo variáveis de execução\n",
    "\n",
    "# env = 'dev'\n",
    "aws_region = 'us-east-2'\n",
    "bucket_s3 = 'ifood-architect-taxi-case'\n",
    "s3_path = f\"s3://{bucket_s3}-{env}-{aws_region}-{aws_account_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ebbb879-280b-4f8b-8cb6-c48a3572ee00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criação da classe NYCData com os atributos respectivos ao objeto \n",
    "\n",
    "class NYCData():\n",
    "\n",
    "    def __init__(self, source: str, dataset_id: str, datetime_col: str) -> None:\n",
    "        self.source = source\n",
    "        self.dataset_id = dataset_id\n",
    "        self.datetime_col = datetime_col\n",
    "    \n",
    "\n",
    "if source_data == 'yellow_tripdata':\n",
    "    nyc_data = NYCData('yellow_tripdata', '4b4i-vvec', 'tpep_pickup_datetime')\n",
    "\n",
    "if source_data == 'green_tripdata':\n",
    "    nyc_data = NYCData('green_tripdata', 'peyi-gg4n', 'lpep_pickup_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f845328-f6d9-45e2-8bdf-b744077b15b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_deterministic_uuid(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Criação de um UUID determinístico a partir de um texto.\n",
    "\n",
    "    Args:\n",
    "        - text(str): texto de origem.\n",
    "\n",
    "    Returns:\n",
    "        str: uuid determinístico.\n",
    "    \"\"\"\n",
    "\n",
    "    return str(uuid.uuid5(uuid.NAMESPACE_OID, text))\n",
    "\n",
    "# Registro da função como UDF\n",
    "\n",
    "uuid5_udf = udf(create_deterministic_uuid, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "772f7643-b6a3-4a68-9edc-7f436ccbefea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Registrando External Table\n",
    "\n",
    "# def register_external_delta_table(\n",
    "#     catalog_name: str, \n",
    "#     schema_name: str, \n",
    "#     table_name: str, \n",
    "#     s3_path: str\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Registro das tabelas externas.\n",
    "\n",
    "#     Args:\n",
    "#         - catalog_name(str): nome do catálogo.\n",
    "#         - schema_name(str): nome do schema.\n",
    "#         - table_name(str): nome da tabela.\n",
    "#         - s3_path(str): caminho do objeto no bucket s3.\n",
    "#     \"\"\"\n",
    "\n",
    "    \n",
    "#     full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "    \n",
    "#     sql_command = f\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS {full_table_name}\n",
    "#     USING DELTA \n",
    "#     LOCATION '{s3_path}/{table_name}'\n",
    "#     \"\"\"\n",
    "    \n",
    "#     print(f\"Registrando tabela: {full_table_name}...\")\n",
    "#     spark.sql(sql_command)\n",
    "#     print(f\"Tabela '{full_table_name}' registrada, apontando para: {s3_path}/{table_name}\")\n",
    "\n",
    "# # Definindo uma lista com os datasets\n",
    "\n",
    "# datasets = [\"green_tripdata\", \"yellow_tripdata\"]\n",
    "\n",
    "# # Iterando sobre os datasets para registro das tabelas externas com os dados NYCOpenData\n",
    "\n",
    "# for dataset in datasets:\n",
    "\n",
    "#     register_external_delta_table(\n",
    "#         catalog_name = \"main\",\n",
    "#         schema_name = \"bronze\",\n",
    "#         table_name = dataset,\n",
    "#         s3_path = s3_path\n",
    "#     )\n",
    "\n",
    "# # Registro da tabela de checkpoint\n",
    "\n",
    "# register_external_delta_table(\n",
    "#     catalog_name = \"main\"\n",
    "#     , schema_name = \"metadata\"\n",
    "#     , table_name = \"socrata_checkpoint\"\n",
    "#     , s3_path = s3_path\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae8d5c7b-d66c-44db-94a9-fb20ab0951f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_last_success_datetime(source: str, initial_datetime: str = '2023-01-01T00:00:00'):\n",
    "\n",
    "    try: \n",
    "\n",
    "        checkpoint_df = spark.table(\"main.metadata.socrata_checkpoint\")\n",
    "\n",
    "        result = (\n",
    "            checkpoint_df\n",
    "            .filter(col(\"pipeline_name\") == source)\n",
    "            .select(\"last_successful_datetime\")\n",
    "        ).collect()\n",
    "\n",
    "        if result and result[0][0]:\n",
    "            print(f\"Checkpoint encontrado para {source}: {result[0][0]}\")\n",
    "            return result[0][0]\n",
    "\n",
    "    except Exception:\n",
    "        pass \n",
    "\n",
    "    print(f\"Nenhum checkpoint encontrado para {source}. Iniciado em {initial_datetime}\")\n",
    "    return initial_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff87a06-0e88-4a77-824f-f099c08b0959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def chunk_generator(generator, chunk_size: int):\n",
    "    chunk = []\n",
    "    for item in generator:\n",
    "        chunk.append(item)\n",
    "        if len(chunk) >= chunk_size:\n",
    "            yield chunk\n",
    "            chunk = []\n",
    "    if chunk:\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "627d1ab3-2f16-4c3f-97e3-7efa15d91435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_new_checkpoint(pipeline_source: str, new_datetime: str, s3_path: str):\n",
    "    \"\"\"\n",
    "    Atualiza o checkpoint para o pipeline usando a operação MERGE INTO.\n",
    "    \"\"\"\n",
    "\n",
    "    checkpoint_schema = StructType([\n",
    "        StructField(\"pipeline_name\", StringType(), False)\n",
    "        , StructField(\"last_successful_datetime\", StringType(), False)\n",
    "        , StructField(\"updated_at\", TimestampType(), False)\n",
    "    ])\n",
    "    \n",
    "    new_checkpoint_df = spark.createDataFrame([\n",
    "        (pipeline_source, new_datetime, datetime.now())\n",
    "    ], checkpoint_schema)\n",
    "    \n",
    "    full_table_name = \"main.metadata.socrata_checkpoint\"\n",
    "    delta_path = f\"{s3_path}/socrata_checkpoint\"\n",
    "\n",
    "    if not spark.catalog.tableExists(full_table_name):\n",
    "        new_checkpoint_df.write.format(\"delta\").mode(\"overwrite\").option(\"path\", delta_path).saveAsTable(full_table_name)\n",
    "    else:\n",
    "        delta_table = DeltaTable.forName(spark, full_table_name)\n",
    "    \n",
    "        delta_table.alias(\"target\") \\\n",
    "            .merge(\n",
    "                source = new_checkpoint_df.alias(\"source\"),\n",
    "                condition = col(\"target.pipeline_name\") == col(\"source.pipeline_name\")\n",
    "            ) \\\n",
    "            .whenMatchedUpdate(set = {\n",
    "                \"last_successful_datetime\": col(\"source.last_successful_datetime\"),\n",
    "                \"updated_at\": col(\"source.updated_at\")\n",
    "            }) \\\n",
    "            .whenNotMatchedInsertAll() \\\n",
    "            .execute()\n",
    "    \n",
    "    print(f\"Checkpoint atualizado com sucesso para: {new_datetime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b51c52b-b767-4aa0-8d6e-df77e393f512",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_and_merge_chunk(chunk_list: list, chunk_index: int, s3_path: str, nyc_data: NYCData):\n",
    "\n",
    "    if not chunk_list:\n",
    "        return\n",
    "\n",
    "    print(f\"Processando Chunk #{chunk_index} com {len(chunk_list)} registros...\")\n",
    "\n",
    "    df = spark.createDataFrame(chunk_list)\n",
    "\n",
    "    df = (\n",
    "        df.withColumns({\n",
    "            'ride_uuid': uuid5_udf(concat_ws(\"||\", col(\"vendorid\"), col(nyc_data.datetime_col), col(\"trip_distance\"), col(\"fare_amount\")))\n",
    "            , 'date_partition': date_format(col(nyc_data.datetime_col), \"yyyy-MM-dd\")\n",
    "            , 'updated_at': now()\n",
    "        })\n",
    "    )\n",
    "\n",
    "    full_table_name = f\"main.bronze.{nyc_data.source}\"\n",
    "    delta_path = f\"{s3_path}/{nyc_data.source}\"\n",
    "\n",
    "    if not spark.catalog.tableExists(full_table_name):\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").option(\"path\", delta_path).partitionBy(\"date_partition\").saveAsTable(full_table_name)\n",
    "    else:\n",
    "        delta_table = DeltaTable.forName(spark, full_table_name)\n",
    "        merge_condition = \"target.ride_uuid = source.ride_uuid\"\n",
    "        \n",
    "        delta_table.alias(\"target\").merge(\n",
    "            source = df.alias(\"source\"),\n",
    "            condition = merge_condition\n",
    "        ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "    max_date_in_chunk = df.selectExpr(f\"MAX({nyc_data.datetime_col})\").collect()[0][0]\n",
    "\n",
    "    save_new_checkpoint(nyc_data.source, max_date_in_chunk, s3_path)\n",
    "\n",
    "    print(f\"Chunk #{chunk_index} salvo com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b738789-be1f-41b7-a13c-521531720910",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ingest_chunks(nyc_data: NYCData, s3_path: str):\n",
    "\n",
    "    chunk_size = 50000\n",
    "    initial_datetime = get_last_success_datetime(nyc_data.source)\n",
    "\n",
    "    client = Socrata(\"data.cityofnewyork.us\", nyc_app_token, timeout=1000)\n",
    "    date_filter = f\"{nyc_data.datetime_col} >= '{initial_datetime}' AND {nyc_data.datetime_col} < '2023-06-01T00:00:00'\"\n",
    "\n",
    "    data_generator = client.get_all(\n",
    "        nyc_data.dataset_id\n",
    "        , where = date_filter\n",
    "        , order = f\"{nyc_data.datetime_col} ASC\"\n",
    "    )\n",
    "\n",
    "    chunk_index = 1\n",
    "    for chunk in chunk_generator(data_generator, chunk_size):\n",
    "        process_and_merge_chunk(chunk, chunk_index, s3_path, nyc_data)\n",
    "        chunk_index += 1\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "print(\"Ingestão completa de todos os chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f412fd5a-34d3-41e4-a687-570eadcf1330",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingest_chunks(nyc_data, s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a54c976-8b16-41f5-aa29-589c31c2eb86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# chunk_size = 50000\n",
    "# initial_datetime = get_last_success_datetime(yellow.source)\n",
    "\n",
    "# client = Socrata(\"data.cityofnewyork.us\", nyc_app_token, timeout=1000)\n",
    "# date_filter = f\"{yellow.datetime_col} >= '{initial_datetime}' AND {yellow.datetime_col} < '2023-06-01T00:00:00'\"\n",
    "\n",
    "# data_generator = client.get_all(\n",
    "#     yellow.dataset_id\n",
    "#     , where = date_filter\n",
    "#     , order = f\"{yellow.datetime_col} ASC\"\n",
    "# )\n",
    "\n",
    "# chunk_index = 1\n",
    "# for chunk in chunk_generator(data_generator, chunk_size):\n",
    "#     process_and_merge_chunk(chunk, chunk_index, s3_path, yellow)\n",
    "#     chunk_index += 1\n",
    "\n",
    "#     time.sleep(5)\n",
    "\n",
    "# print(\"Ingestão completa de todos os chunks.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6824569427356789,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_data_from_api",
   "widgets": {
    "env": {
     "currentValue": "dev",
     "nuid": "62d2a0eb-e283-47f3-812e-4e4e13b82cc7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Execution Environment",
      "name": "env",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "prod"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Execution Environment",
      "name": "env",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dev",
        "prod"
       ]
      }
     }
    },
    "source_data": {
     "currentValue": "yellow_tripdata",
     "nuid": "927b83df-12c8-481c-ba6f-faa8c5c50ff7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "yellow_tripdata",
      "label": "NYCOpenData",
      "name": "source_data",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "yellow_tripdata",
        "green_tripdata"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "yellow_tripdata",
      "label": "NYCOpenData",
      "name": "source_data",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "yellow_tripdata",
        "green_tripdata"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
